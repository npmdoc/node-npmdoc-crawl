<div class="apidocDiv">
<style>
/*csslint
*/
.apidocDiv {
    background: #fff;
    font-family: Arial, Helvetica, sans-serif;
}
.apidocDiv a[href] {
    color: #33f;
    font-weight: bold;
    text-decoration: none;
}
.apidocDiv a[href]:hover {
    text-decoration: underline;
}
.apidocCodeCommentSpan {
    background: #bbf;
    color: #000;
    display: block;
}
.apidocCodeKeywordSpan {
    color: #d00;
    font-weight: bold;
}
.apidocCodePre {
    background: #eef;
    border: 1px solid;
    color: #777;
    padding: 5px;
    white-space: pre-wrap;
}
.apidocFooterDiv {
    margin-top: 20px;
    text-align: center;
}
.apidocModuleLi {
    margin-top: 10px;
}
.apidocSectionDiv {
    border-top: 1px solid;
    margin-top: 20px;
}
.apidocSignatureSpan {
    color: #777;
    font-weight: bold;
}
</style>
<h1>api documentation for
    <a

        href="https://github.com/mmoulton/crawl#readme"

    >crawl (v0.3.1)</a>
</h1>
<h4>Website crawler and differencer</h4>
<div class="apidocSectionDiv"><a
    href="#apidoc.tableOfContents"
    id="apidoc.tableOfContents"
><h1>table of contents</h1></a><ol>

    <li class="apidocModuleLi"><a href="#apidoc.module.crawl">module crawl</a><ol>

        <li>

            <a class="apidocElementLiA" href="#apidoc.element.crawl.crawl">
            function <span class="apidocSignatureSpan"></span>crawl
            <span class="apidocSignatureSpan">(url, options, callback)</span>
            </a>

        </li>

        <li>

            <a class="apidocElementLiA" href="#apidoc.element.crawl.diff">
            function <span class="apidocSignatureSpan">crawl.</span>diff
            <span class="apidocSignatureSpan">(leftUrl, rightUrl, options, callback)</span>
            </a>

        </li>

        <li>

            <span class="apidocSignatureSpan">object <span class="apidocSignatureSpan">crawl.</span>crawler</span>

        </li>

        <li>

            <span class="apidocSignatureSpan">object <span class="apidocSignatureSpan">crawl.</span>differencer</span>

        </li>

    </ol></li>

    <li class="apidocModuleLi"><a href="#apidoc.module.crawl.crawler">module crawl.crawler</a><ol>

        <li>

            <a class="apidocElementLiA" href="#apidoc.element.crawl.crawler.crawl">
            function <span class="apidocSignatureSpan">crawl.crawler.</span>crawl
            <span class="apidocSignatureSpan">(url, options, callback)</span>
            </a>

        </li>

        <li>

            <span class="apidocSignatureSpan">number <span class="apidocSignatureSpan">crawl.crawler.</span>_eventsCount</span>

        </li>

        <li>

            <span class="apidocSignatureSpan">object <span class="apidocSignatureSpan">crawl.crawler.</span>_events</span>

        </li>

        <li>

            <span class="apidocSignatureSpan">object <span class="apidocSignatureSpan">crawl.crawler.</span>domain</span>

        </li>

    </ol></li>

    <li class="apidocModuleLi"><a href="#apidoc.module.crawl.differencer">module crawl.differencer</a><ol>

        <li>

            <a class="apidocElementLiA" href="#apidoc.element.crawl.differencer.diff">
            function <span class="apidocSignatureSpan">crawl.differencer.</span>diff
            <span class="apidocSignatureSpan">(leftUrl, rightUrl, options, callback)</span>
            </a>

        </li>

        <li>

            <span class="apidocSignatureSpan">number <span class="apidocSignatureSpan">crawl.differencer.</span>_eventsCount</span>

        </li>

        <li>

            <span class="apidocSignatureSpan">object <span class="apidocSignatureSpan">crawl.differencer.</span>_events</span>

        </li>

        <li>

            <span class="apidocSignatureSpan">object <span class="apidocSignatureSpan">crawl.differencer.</span>domain</span>

        </li>

    </ol></li>

</ol></div>

<div class="apidocSectionDiv">
<h1><a href="#apidoc.module.crawl" id="apidoc.module.crawl">module crawl</a></h1>


    <h2>
        <a href="#apidoc.element.crawl.crawl" id="apidoc.element.crawl.crawl">
        function <span class="apidocSignatureSpan"></span>crawl
        <span class="apidocSignatureSpan">(url, options, callback)</span>
        </a>
    </h2>
    <ul>
    <li>description and source-code<pre class="apidocCodePre">crawl = function (url, options, callback) {
  if (typeof options === &#x27;function&#x27;) {
    callback = options;
    options = {};
  }
  options = options || {};

  // setup some default options for the node.io job
  options[&#x27;crawler&#x27;] = crawler;

	var pages = {},
      potentialPages = {},
      urlParts = urlUtil.parse(url, true);

  // do a web crawl of url if it&#x27;s an http protocol
	if (urlParts.protocol == &#x22;https:&#x22; || urlParts.protocol == &#x22;http:&#x22;) {

    var port = urlParts.port ? urlParts.port : 80,
        siteCrawler = new Crawler(urlParts.hostname, urlParts.path, port);

    // configure crawler
    siteCrawler.interval = 10;
    siteCrawler.maxConcurrency = 10;
    siteCrawler.scanSubdomains = true;
    siteCrawler.downloadUnsupported = false;

    if (options.username &#x26;&#x26; options.password) {
      siteCrawler.needsAuth = true;
      siteCrawler.authUser = options.username;
      siteCrawler.authPass = options.password;
    }

    function mimeTypeSupported(MIMEType) {
      var supported = false;
      siteCrawler.supportedMimeTypes.forEach(function(mimeCheck) {
        if (!!mimeCheck.exec(MIMEType)) {
          supported = true;
        }
      });
      return supported;
    }

    var pageHandler = function(queueItem, responseBuffer, response) {
      if (mimeTypeSupported(queueItem.stateData.contentType)) {
        crawler.emit(&#x22;crawl&#x22;, queueItem.url);

        var data = responseBuffer.toString(),
            parsedUrl = urlUtil.parse(queueItem.url, true),
            page = {};

        page.url = queueItem.url;
        page.status = queueItem.stateData.code;
        page.contentType = queueItem.stateData.contentType;
        page.checksum = hash.sha256(data);
        page.date = new Date().toJSON();

        if (options.headers) page.headers = queueItem.stateData.headers;
        if (options.body) page.body = data;

        if (potentialPages[parsedUrl.path]) {
          page = _.extend(page, potentialPages[parsedUrl.path]);
        }

        pages[parsedUrl.path] = page;
      }
    };

    siteCrawler.on(&#x22;discoverycomplete&#x22;, function(queueItem, resources) {
      var parsedUrl = urlUtil.parse(queueItem.url, true);

      // Save the outbound links for the item that just completed discovery
      pages[parsedUrl.path].links = resources;

      // Update each linked to page storing us as a referrer
      resources.forEach(function(link) {
        // A normalized, resolved URL, used for uniq identification purposes
        var resourceUrl = urlUtil.parse(urlUtil.resolve(parsedUrl, urlUtil.parse(link, true)), true);

        // Links found in the discovery of this resource may not have been crawled yet.
        // In the case of links that have not yet been crawled, we save them as potential
        // pages that may meet the crawling criteria (content-type,etc).
        if (!pages[resourceUrl.path]) {
          if (!potentialPages[resourceUrl.path]) potentialPages[resourceUrl.path] = { referrers: [] };
          potentialPages[resourceUrl.path].referrers.push(queueItem.url);

        // Otherwise the resourece has already been crawled so we can store ourselves as a referrer.
        } else {
          if (!pages[resourceUrl.path].referrers) pages[resourceUrl.path].referrers = [];
          pages[resourceUrl.path].referrers.push(queueItem.url);
        }
      });
    });

    // handle normal pages
    siteCrawler.on(&#x22;fetchcomplete&#x22;, pageHandler);

    // handle broken links
    siteCrawler.on(&#x22;fetch404&#x22;, pageHandler);

    // on completion, parse broken links and report
    siteCrawler.on(&#x22;complete&#x22;, function(queueItem, responseBuffer, response) {
      callback(null, _.map(pages, function (value, key) {
        // De-Dup referrers. Note: this could be done much more efficiently if performance
        // becomes a problem.
        if (value.referrers) value.referrers = _.uniq(value.referrers);
        return value;
      }));
    });

    siteCrawler.start(); // crawl the site

	}

  // otherwise we load a json file, assumed to be generated by the CLI using the &#x27;--json&#x27; option
  else if (urlParts.path) {
    var path ...</pre></li>
    <li>example usage<pre class="apidocCodePre">...

## Programatic Use

Crawl is rather simple to use in your own code. Simple include crawl as a dependency of your project (add to package.json, etc)
then integrate like so:

	var crawl = require(&#x27;crawl&#x27;);

	crawl.<span class="apidocCodeKeywordSpan">crawl</span>(&#x22;http://your-domain.com&#x22;, function(err, pages) {
		if (err) {
			console.error(&#x22;An error occured&#x22;, err);
			return;
		}

		console.log(JSON.stringify(pages));
	});
...</pre></li>
    </ul>



    <h2>
        <a href="#apidoc.element.crawl.diff" id="apidoc.element.crawl.diff">
        function <span class="apidocSignatureSpan">crawl.</span>diff
        <span class="apidocSignatureSpan">(leftUrl, rightUrl, options, callback)</span>
        </a>
    </h2>
    <ul>
    <li>description and source-code<pre class="apidocCodePre">diff = function (leftUrl, rightUrl, options, callback) {
  if (typeof options === &#x27;function&#x27;) {
    callback = options;
    options = {};
  }
  options = options || {};

  var leftPages, rightPages;

  first(function() {
    var that = this;
    crawler.crawl(leftUrl, options, function(err, pages) {
      if (err) callback(err);

      leftPages = pages;
      that();
    });
  })
  .whilst(function() {
    var that = this;
    crawler.crawl(rightUrl, options, function(err, pages) {
      if (err) callback(err);

      rightPages = pages;
      that();
    });
  })
  .then(function() {
    // perform diff
    var differences = {};
    analyze(leftPages, true, options, differences);
    analyze(rightPages, false, options, differences);
    callback(null, leftPages, rightPages, differences);
  });

}</pre></li>
    <li>example usage<pre class="apidocCodePre">n/a</pre></li>
    </ul>






</div>

<div class="apidocSectionDiv">
<h1><a href="#apidoc.module.crawl.crawler" id="apidoc.module.crawl.crawler">module crawl.crawler</a></h1>


    <h2>
        <a href="#apidoc.element.crawl.crawler.crawl" id="apidoc.element.crawl.crawler.crawl">
        function <span class="apidocSignatureSpan">crawl.crawler.</span>crawl
        <span class="apidocSignatureSpan">(url, options, callback)</span>
        </a>
    </h2>
    <ul>
    <li>description and source-code<pre class="apidocCodePre">crawl = function (url, options, callback) {
  if (typeof options === &#x27;function&#x27;) {
    callback = options;
    options = {};
  }
  options = options || {};

  // setup some default options for the node.io job
  options[&#x27;crawler&#x27;] = crawler;

	var pages = {},
      potentialPages = {},
      urlParts = urlUtil.parse(url, true);

  // do a web crawl of url if it&#x27;s an http protocol
	if (urlParts.protocol == &#x22;https:&#x22; || urlParts.protocol == &#x22;http:&#x22;) {

    var port = urlParts.port ? urlParts.port : 80,
        siteCrawler = new Crawler(urlParts.hostname, urlParts.path, port);

    // configure crawler
    siteCrawler.interval = 10;
    siteCrawler.maxConcurrency = 10;
    siteCrawler.scanSubdomains = true;
    siteCrawler.downloadUnsupported = false;

    if (options.username &#x26;&#x26; options.password) {
      siteCrawler.needsAuth = true;
      siteCrawler.authUser = options.username;
      siteCrawler.authPass = options.password;
    }

    function mimeTypeSupported(MIMEType) {
      var supported = false;
      siteCrawler.supportedMimeTypes.forEach(function(mimeCheck) {
        if (!!mimeCheck.exec(MIMEType)) {
          supported = true;
        }
      });
      return supported;
    }

    var pageHandler = function(queueItem, responseBuffer, response) {
      if (mimeTypeSupported(queueItem.stateData.contentType)) {
        crawler.emit(&#x22;crawl&#x22;, queueItem.url);

        var data = responseBuffer.toString(),
            parsedUrl = urlUtil.parse(queueItem.url, true),
            page = {};

        page.url = queueItem.url;
        page.status = queueItem.stateData.code;
        page.contentType = queueItem.stateData.contentType;
        page.checksum = hash.sha256(data);
        page.date = new Date().toJSON();

        if (options.headers) page.headers = queueItem.stateData.headers;
        if (options.body) page.body = data;

        if (potentialPages[parsedUrl.path]) {
          page = _.extend(page, potentialPages[parsedUrl.path]);
        }

        pages[parsedUrl.path] = page;
      }
    };

    siteCrawler.on(&#x22;discoverycomplete&#x22;, function(queueItem, resources) {
      var parsedUrl = urlUtil.parse(queueItem.url, true);

      // Save the outbound links for the item that just completed discovery
      pages[parsedUrl.path].links = resources;

      // Update each linked to page storing us as a referrer
      resources.forEach(function(link) {
        // A normalized, resolved URL, used for uniq identification purposes
        var resourceUrl = urlUtil.parse(urlUtil.resolve(parsedUrl, urlUtil.parse(link, true)), true);

        // Links found in the discovery of this resource may not have been crawled yet.
        // In the case of links that have not yet been crawled, we save them as potential
        // pages that may meet the crawling criteria (content-type,etc).
        if (!pages[resourceUrl.path]) {
          if (!potentialPages[resourceUrl.path]) potentialPages[resourceUrl.path] = { referrers: [] };
          potentialPages[resourceUrl.path].referrers.push(queueItem.url);

        // Otherwise the resourece has already been crawled so we can store ourselves as a referrer.
        } else {
          if (!pages[resourceUrl.path].referrers) pages[resourceUrl.path].referrers = [];
          pages[resourceUrl.path].referrers.push(queueItem.url);
        }
      });
    });

    // handle normal pages
    siteCrawler.on(&#x22;fetchcomplete&#x22;, pageHandler);

    // handle broken links
    siteCrawler.on(&#x22;fetch404&#x22;, pageHandler);

    // on completion, parse broken links and report
    siteCrawler.on(&#x22;complete&#x22;, function(queueItem, responseBuffer, response) {
      callback(null, _.map(pages, function (value, key) {
        // De-Dup referrers. Note: this could be done much more efficiently if performance
        // becomes a problem.
        if (value.referrers) value.referrers = _.uniq(value.referrers);
        return value;
      }));
    });

    siteCrawler.start(); // crawl the site

	}

  // otherwise we load a json file, assumed to be generated by the CLI using the &#x27;--json&#x27; option
  else if (urlParts.path) {
    var path ...</pre></li>
    <li>example usage<pre class="apidocCodePre">...

## Programatic Use

Crawl is rather simple to use in your own code. Simple include crawl as a dependency of your project (add to package.json, etc)
then integrate like so:

	var crawl = require(&#x27;crawl&#x27;);

	crawl.<span class="apidocCodeKeywordSpan">crawl</span>(&#x22;http://your-domain.com&#x22;, function(err, pages) {
		if (err) {
			console.error(&#x22;An error occured&#x22;, err);
			return;
		}

		console.log(JSON.stringify(pages));
	});
...</pre></li>
    </ul>








</div>

<div class="apidocSectionDiv">
<h1><a href="#apidoc.module.crawl.differencer" id="apidoc.module.crawl.differencer">module crawl.differencer</a></h1>


    <h2>
        <a href="#apidoc.element.crawl.differencer.diff" id="apidoc.element.crawl.differencer.diff">
        function <span class="apidocSignatureSpan">crawl.differencer.</span>diff
        <span class="apidocSignatureSpan">(leftUrl, rightUrl, options, callback)</span>
        </a>
    </h2>
    <ul>
    <li>description and source-code<pre class="apidocCodePre">diff = function (leftUrl, rightUrl, options, callback) {
  if (typeof options === &#x27;function&#x27;) {
    callback = options;
    options = {};
  }
  options = options || {};

  var leftPages, rightPages;

  first(function() {
    var that = this;
    crawler.crawl(leftUrl, options, function(err, pages) {
      if (err) callback(err);

      leftPages = pages;
      that();
    });
  })
  .whilst(function() {
    var that = this;
    crawler.crawl(rightUrl, options, function(err, pages) {
      if (err) callback(err);

      rightPages = pages;
      that();
    });
  })
  .then(function() {
    // perform diff
    var differences = {};
    analyze(leftPages, true, options, differences);
    analyze(rightPages, false, options, differences);
    callback(null, leftPages, rightPages, differences);
  });

}</pre></li>
    <li>example usage<pre class="apidocCodePre">n/a</pre></li>
    </ul>








</div>

<div class="apidocFooterDiv">
    [ this document was created with
    <a href="https://github.com/kaizhu256/node-utility2" target="_blank">utility2</a>
    ]
</div>
</div>
